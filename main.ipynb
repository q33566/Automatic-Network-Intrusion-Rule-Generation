{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函數有更新的時候會自動重新載入函數的指令\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#-------------------------------------\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import random\n",
    "#from API.FileReader import readkl, readCsv, readGeneratedRegex\n",
    "from API.FileProcessor import merge_dataframes_to_dict, prompt_generator, get_grouped_sid_to_regex_dict, prompt_generator_no_random\n",
    "from API.regexEvaluation import positive_evaluation, negative_evaluation\n",
    "from API.Drawer import draw_outcome_histogram, draw_outcome_bar\n",
    "from API.Drawer import draw_positive_contribution_of_generated_regex, draw_negative_contribution_of_generated_regex\n",
    "from share_data import sharedData\n",
    "from API.FileReader import read_pkl, read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 找出SID有幾個unique text還有占比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_text_counts = imap_df.groupby('sid')['text'].nunique()\n",
    "\n",
    "# Filter to include only those 'sid' with five or more unique 'text' values\n",
    "filtered_sids = unique_text_counts[unique_text_counts >= 5].index\n",
    "\n",
    "# Filter the original DataFrame to include only the rows with the filtered 'sid' values\n",
    "filtered_imap_df = imap_df[imap_df['sid'].isin(filtered_sids)]\n",
    "\n",
    "# Count the total number of 'text' occurrences for each filtered 'sid'\n",
    "sid_text_counts = filtered_imap_df.groupby('sid').size().reset_index(name='text_count')\n",
    "\n",
    "# Calculate the total number of 'text' occurrences in the entire dataset\n",
    "total_texts = imap_df.shape[0]\n",
    "\n",
    "for sid, text_count in zip(sid_text_counts['sid'], sid_text_counts['text_count']):\n",
    "    print(f\"SID: {sid}, Percentage: {text_count/total_texts}\")\n",
    "\n",
    "# Display the results\n",
    "print(sid_text_counts)\n",
    "print(f\"Total number of 'text' occurrences in the dataset: {total_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "#imap_df = pd.read_csv('imap.csv')\n",
    "\n",
    "# Group by 'sid' and count unique texts per 'sid'\n",
    "#unique_text_counts = imap_df.groupby('sid')['text'].nunique()\n",
    "\n",
    "# Filter 'sids' with 5 or more unique texts\n",
    "#filtered_sids = unique_text_counts[unique_text_counts >= 5].index\n",
    "\n",
    "# Create a DataFrame that includes only filtered 'sids'\n",
    "#filtered_imap_df = imap_df[imap_df['sid'].isin(filtered_sids)]\n",
    "\n",
    "# Count total occurrences of 'text' for each 'sid' in the filtered DataFrame\n",
    "sid_text_counts = imap_df.groupby('sid').size().reset_index(name='text_count')\n",
    "\n",
    "# Calculate the total number of 'text' occurrences in the entire dataset\n",
    "total_texts = imap_df.shape[0]\n",
    "\n",
    "# Iterate through each 'sid' in the filtered list and print the percentage of text occurrences\n",
    "for sid, text_count in zip(sid_text_counts['sid'], sid_text_counts['text_count']):\n",
    "    print(f\"SID: {sid}, Percentage: {text_count / total_texts * 100:.2f}%\")\n",
    "\n",
    "# Print the count data\n",
    "print(sid_text_counts)\n",
    "\n",
    "print(f\"Total number of 'text' occurrences in the dataset: {total_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'sid' and count occurrences\n",
    "sid_text_counts = imap_df.groupby('sid').size().reset_index(name='text_count')\n",
    "\n",
    "# Calculate the total number of 'text' occurrences in the entire dataset\n",
    "total_texts = imap_df.shape[0]\n",
    "\n",
    "# Calculate unique text counts per sid\n",
    "unique_text_counts = imap_df.groupby('sid')['text'].nunique().reset_index(name='unique_text_count')\n",
    "\n",
    "# Join unique text counts to sid text counts DataFrame\n",
    "sid_text_counts = sid_text_counts.merge(unique_text_counts, on='sid')\n",
    "\n",
    "# Calculate percentage for better sorting\n",
    "sid_text_counts['Percentage'] = sid_text_counts['text_count'] / total_texts * 100\n",
    "\n",
    "# # Sort by 'sid'\n",
    "# sorted_by_sid = sid_text_counts.sort_values(by='sid')\n",
    "# print(\"Sorted by SID:\")\n",
    "# print(sorted_by_sid)\n",
    "\n",
    "# Sort by 'Percentage' of text occurrences\n",
    "sorted_by_percentage = sid_text_counts.sort_values(by='Percentage', ascending=False)\n",
    "print(\"\\nSorted by Percentage of Text Occurrences:\")\n",
    "print(sorted_by_percentage)\n",
    "\n",
    "# # Sort by 'unique_text_count'\n",
    "# sorted_by_unique_texts = sid_text_counts.sort_values(by='unique_text_count', ascending=False)\n",
    "# print(\"\\nSorted by Number of Unique Texts:\")\n",
    "# print(sorted_by_unique_texts)\n",
    "\n",
    "# Print the total number of 'text' occurrences in the dataset\n",
    "#print(f\"\\nTotal number of 'text' occurrences in the dataset: {total_texts}\")\n",
    "print()\n",
    "num_sid_zero_unique_texts = (sid_text_counts['unique_text_count'] == 0).sum()\n",
    "print(f\"Number of SIDs with zero unique texts: {num_sid_zero_unique_texts}\")\n",
    "total_sids = imap_df['sid'].nunique()\n",
    "print(f\"Total number of unique SIDs: {total_sids}\")\n",
    "total_sids = len(imap_df['sid'])\n",
    "num_rows_with_no_text = imap_df['text'].isnull().sum()\n",
    "print('imap_df')\n",
    "print(f\"Total number of binary (including duplicates): {num_rows_with_no_text}\")\n",
    "print(f\"Total number of SIDs (including duplicates): {total_sids}\")\n",
    "print(f\"{num_rows_with_no_text/total_sids*100:.2f}% of rows have no text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs('./df_data', exist_ok=True)\n",
    "\n",
    "# List of DataFrames to iterate through\n",
    "dataframes = {\n",
    "    'pop_df': pop_df,\n",
    "    'imap_df': imap_df,\n",
    "    'smtp_df': smtp_df,\n",
    "    'sip_df': sip_df\n",
    "}\n",
    "\n",
    "# Process each DataFrame\n",
    "for name, df in dataframes.items():\n",
    "    # Group by 'sid' and count occurrences\n",
    "    sid_text_counts = df.groupby('sid').size().reset_index(name='text_count')\n",
    "\n",
    "    # Calculate the total number of 'text' occurrences in the entire dataset\n",
    "    total_texts = df.shape[0]\n",
    "\n",
    "    # Calculate unique text counts per sid\n",
    "    unique_text_counts = df.groupby('sid')['text'].nunique().reset_index(name='unique_text_count')\n",
    "\n",
    "    # Join unique text counts to sid text counts DataFrame\n",
    "    sid_text_counts = sid_text_counts.merge(unique_text_counts, on='sid')\n",
    "\n",
    "    # Calculate percentage for better sorting\n",
    "    sid_text_counts['Percentage'] = sid_text_counts['text_count'] / total_texts * 100\n",
    "\n",
    "    # Sort by 'Percentage' of text occurrences\n",
    "    sorted_by_percentage = sid_text_counts.sort_values(by='Percentage', ascending=False)\n",
    "\n",
    "    # Prepare the results to be written to the text file\n",
    "    result = []\n",
    "    result.append(f\"\\nSorted by Percentage of Text Occurrences ({name}):\")\n",
    "    result.append(sorted_by_percentage.to_string(index=False))\n",
    "    num_sid_zero_unique_texts = (sid_text_counts['unique_text_count'] == 0).sum()\n",
    "    total_sids = df['sid'].nunique()\n",
    "    num_rows_with_no_text = df['text'].isnull().sum()\n",
    "    total_sids_including_duplicates = len(df['sid'])\n",
    "\n",
    "    result.append(f\"\\nNumber of SIDs with zero unique texts: {num_sid_zero_unique_texts}\")\n",
    "    result.append(f\"Total number of unique SIDs: {total_sids}\")\n",
    "    result.append(f\"Total number of SIDs (including duplicates): {total_sids_including_duplicates}\")\n",
    "    result.append(f\"Total number of binary (including duplicates): {num_rows_with_no_text}\")\n",
    "    result.append(f\"{num_rows_with_no_text/total_sids_including_duplicates*100:.2f}% of rows have no text\")\n",
    "\n",
    "    # Identify SIDs with 5 or more unique texts\n",
    "    sids_with_5_or_more_unique_texts = unique_text_counts[unique_text_counts['unique_text_count'] >= 5]\n",
    "\n",
    "    result.append(f\"\\nSIDs with 5 or more unique texts ({name}):\")\n",
    "    result.append(sids_with_5_or_more_unique_texts.to_string(index=False))\n",
    "\n",
    "    # Write the results to a text file\n",
    "    with open(f'./df_data/{name}_df.txt', 'w') as file:\n",
    "        file.write('\\n'.join(result))\n",
    "\n",
    "print(\"Processing complete. Results stored in ./df_data/ directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = read_pkl(\"dataset/pop_report_with_tknscore_new.pkl\")\n",
    "imap_df = read_pkl(\"dataset/imap_report_with_tknscore_new.pkl\")\n",
    "smtp_df = read_pkl(\"dataset/smtp_report_with_tknscore_new.pkl\")\n",
    "sip_df = read_pkl(\"dataset/sip_report_with_tknscore_new.pkl\")\n",
    "label_df = read_csv(\"dataset/sid_table(packet).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r\"C:\\NCU\\Intern\\AutomaticNetwork IntrusionRuleGeneration\\experiment\\1 out of 9\\generated regex\"\n",
    "\n",
    "shared_data = sharedData(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered_sids_to_unique_texts: pd.DataFrame = shared_data.pop_df.groupby('sid')['text'].apply(lambda x: list(set(x.dropna()))).reset_index(name='texts')\n",
    "filtered_sids_to_unique_texts: pd.DataFrame = unfiltered_sids_to_unique_texts[unfiltered_sids_to_unique_texts['texts'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "print(type(filtered_sids_to_unique_texts))\n",
    "\n",
    "#for sid, texts in zip(filtered_sids_to_unique_texts['sid'], filtered_sids_to_unique_texts['texts']):\n",
    "#    print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將所有資料集轉換為dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pop_df, imap_df, smtp_df, sip_df]\n",
    "sid_to_unique_texts_dict = merge_dataframes_to_dict(dfs)\n",
    "sid_to_unique_texts_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實驗參數初始化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_ONE_REGEX_GROUP = 9\n",
    "EXPERIMENT_NAME = \"1 out of 9\"\n",
    "PROCESS_TYPE = \"with post-process\" #with/without post-process\n",
    "MODE = \"fraction\" #fraction/integer\n",
    "THRESHOLD = 2/3\n",
    "#escaped_string = original_string.replace('\"', '\\\\\"')\n",
    "if not os.path.exists(f\"experiment/{EXPERIMENT_NAME}\"):\n",
    "    os.makedirs(f\"experiment/{EXPERIMENT_NAME}\")\n",
    "\n",
    "#讀取csv 每個row是一個regex begin\n",
    "#generated_regex_list: list = readGeneratedRegex(f\"experiment/{EXPERIMENT_NAME}/generated regex.csv\")\n",
    "#grouped_sid_to_regex_dict = get_grouped_sid_to_regex_dict(generated_regex_list, 9, sid_to_unique_texts_dict)\n",
    "#讀取csv 每個row是一個regex end\n",
    "\n",
    "#讀取csv 每個row是一個dict+後處理 begin\n",
    "#generated_regex_list = cleanAllNewlineinList(generated_regex_list)\n",
    "#sid_to_unique_texts_dict = cleanAllNewlineinDict(sid_to_unique_texts_dict)\n",
    "#grouped_sid_to_regex_dict = merged_cleaned_regex_dictionary\n",
    "#讀取csv 每個row是一個dict+後處理 end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "p = 'DELE 97'\n",
    "bool(regex.search(pattern, target_text, regex.DOTALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sid                                              regex\n",
      "0  1161912  [^DELE \\d{1,3}\\r\\n$, ^DELE [1-9][0-9]{0,2}\\r\\n...\n",
      "1  1176066  [^(RETR \\d{1,3}\\r\\n)$, ^(TOP \\d{1,3} 0\\r\\n)$, ...\n",
      "2  1239754  [^GET http:\\/\\/clientapi\\.ipip\\.net\\/echo\\.php...\n",
      "3  1454621  [^EHLO\\s[\\w\\.-]+\\r\\n$, ^EHLO\\s(\\d{1,3}\\.){3}\\d...\n",
      "4  1491662  [^(POST /ipp HTTP/1\\.1\\r\\nHost: [0-9]+\\.[0-9]+...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the directory containing the text files\n",
    "directory_path = r\"C:\\NCU\\Intern\\AutomaticNetwork IntrusionRuleGeneration\\experiment\\1 out of 9\\generated regex\"\n",
    "\n",
    "# # Dictionary to hold the file contents\n",
    "# file_contents = {}\n",
    "\n",
    "# # Read each file in the directory and store its contents in the dictionary\n",
    "# for filename in os.listdir(directory_path):\n",
    "#     file_path = os.path.join(directory_path, filename)\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         file_contents[filename] = file.readlines()\n",
    "\n",
    "# # Remove the '.txt' extension from the keys in the dictionary\n",
    "# file_contents_no_ext = {filename[:-4]: contents for filename, contents in file_contents.items()}\n",
    "\n",
    "# # Print the dictionary\n",
    "# print(file_contents_no_ext)\n",
    "print(shared_data.get_generated_regex().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_sid_to_regex_dict)\n",
    "print(list(grouped_sid_to_regex_dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./experiment/{EXPERIMENT_NAME}/graph/{PROCESS_TYPE}'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_generator_no_random(sid_to_unique_texts_dict, EXPERIMENT_NAME, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 執行正樣本比對與負樣本拒絕測試\n",
    "- 將結果儲存於./experiment/{EXPERIMENT_NAME}/data/{PROCESS_TYPE}/sid.txt \n",
    "- 計算positive_sid_to_success_rate_dict, positive_text_to_error_regex_dict，negative_sid_to_seccess_rate_dict，negative_text_to_error_regex_dict用於後續的作圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "positive_sid_to_success_rate_dict = {}\n",
    "negative_sid_to_success_rate_dict = {}\n",
    "positive_text_to_error_regex_dict = {}\n",
    "negative_text_to_error_regex_dict = {}\n",
    "directory = f'./experiment/{EXPERIMENT_NAME}/data/{PROCESS_TYPE}'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "# post sementic process\n",
    "for sid, regex_list in grouped_sid_to_regex_dict.items():\n",
    "    transformed_data_step1 = [entry.rstrip('$\\n') for entry in grouped_sid_to_regex_dict[sid]]\n",
    "    transformed_data_step2 = [entry.replace('\\\\r\\\\n', '\\r\\n') for entry in transformed_data_step1]\n",
    "    grouped_sid_to_regex_dict[sid] = transformed_data_step2\n",
    "for sid, regex_list in grouped_sid_to_regex_dict.items():   \n",
    "    with open(f'{directory}/{sid}.txt', 'w') as file:\n",
    "        positive_evaluation(regex_list, sid, sid_to_unique_texts_dict, file, positive_sid_to_success_rate_dict, positive_text_to_error_regex_dict, THRESHOLD ,MODE)\n",
    "        negative_evaluation(regex_list, sid, sid_to_unique_texts_dict, file, negative_sid_to_success_rate_dict, negative_text_to_error_regex_dict, THRESHOLD, MODE)\n",
    "\n",
    "# transformed_data_step1 = [entry.rstrip('$\\n') for entry in grouped_sid_to_regex_dict['1161912']]\n",
    "# transformed_data_step2 = [entry.replace('\\\\r\\\\n', '\\r\\n') for entry in transformed_data_step1]\n",
    "# grouped_sid_to_regex_dict['1161912'] = transformed_data_step2\n",
    "# with open(f'{directory}/{1161912}.txt', 'w') as file:\n",
    "#     positive_evaluation(transformed_data_step2, '1161912', sid_to_unique_texts_dict, file, positive_sid_to_success_rate_dict, positive_text_to_error_regex_dict, THRESHOLD ,MODE)\n",
    "#     negative_evaluation(grouped_sid_to_regex_dict['1161912'], '1161912', sid_to_unique_texts_dict, file, negative_sid_to_success_rate_dict, negative_text_to_error_regex_dict, THRESHOLD, MODE)\n",
    "#後處理 \"前面加\\\n",
    "#把首尾的/去掉\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算貢獻度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 sid_to_unique_texts_dict\n",
    "#sid_to_unique_texts_dict = {sid: set(texts) for sid, texts in grouped_sid_to_regex_dict.items()}\n",
    "# 新的結果字典\n",
    "positive_expand_dict = draw_positive_contribution_of_generated_regex(grouped_sid_to_regex_dict, sid_to_unique_texts_dict, positive_text_to_error_regex_dict, EXPERIMENT_NAME)\n",
    "negative_expand_dict = draw_negative_contribution_of_generated_regex(grouped_sid_to_regex_dict, sid_to_unique_texts_dict, negative_text_to_error_regex_dict, EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_hiest_score_dict(expand_dict):\n",
    "    result = {}\n",
    "    for key, value in expand_dict.items():\n",
    "        text_list = value['text list']\n",
    "        max_text = max(text_list, key=text_list.get)\n",
    "        max_score = text_list[max_text]\n",
    "        result[key] = {'text': max_text, 'score': max_score}\n",
    "    return result\n",
    "\n",
    "positive_highest_score_dict = get_hiest_score_dict(positive_expand_dict)\n",
    "negative_highest_score_dict = get_hiest_score_dict(negative_expand_dict)\n",
    "\n",
    "# Write positive_expand_dict in a human-readable format\n",
    "with open(f'./experiment/{EXPERIMENT_NAME}/data/{PROCESS_TYPE}/positive_expand_dict.txt', 'w') as file:\n",
    "    json.dump(positive_highest_score_dict, file, indent=4)\n",
    "\n",
    "# Write negative_expand_dict in a human-readable format\n",
    "with open(f'./experiment/{EXPERIMENT_NAME}/data/{PROCESS_TYPE}/negative_expand_dict.txt', 'w') as file:\n",
    "    json.dump(negative_highest_score_dict, file, indent=4)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_highest_scores(highest_score_dict, title, save_path):\n",
    "    ids = list(highest_score_dict.keys())\n",
    "    scores = [highest_score_dict[id][\"score\"] for id in ids]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(ids, scores, color='skyblue')\n",
    "    plt.xlabel('ID Key')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(title)\n",
    "    plt.ylim(0, 1.1)  # Limit y-axis to 110% to provide space for annotations\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Annotate bars with percentage values\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f'{height*100:.1f}%',\n",
    "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3),  # 3 points vertical offset\n",
    "                     textcoords=\"offset points\",\n",
    "                     ha='center', va='bottom',\n",
    "                     rotation=45)  # Rotate the text to prevent overlap\n",
    "\n",
    "    # Add padding to the top of the plot to prevent title overlap\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "\n",
    "    # Save the plot\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_path, f'{title}.png'))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_highest_scores(positive_highest_score_dict, f'Positive Highest Scores {PROCESS_TYPE}', save_path=f'./experiment/{EXPERIMENT_NAME}/graph/{PROCESS_TYPE}')\n",
    "plot_highest_scores(negative_highest_score_dict, f'Negative Highest Scores {PROCESS_TYPE}', save_path=f'./experiment/{EXPERIMENT_NAME}/graph/{PROCESS_TYPE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = os.path.join('./experiment', 'test', 'graph/contribution/positive')\n",
    "# os.makedirs(base_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sid_to_success_rate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_outcome_bar(positive_sid_to_success_rate_dict,EXPERIMENT_NAME, f\"Positive Accuracy {PROCESS_TYPE}\")\n",
    "draw_outcome_histogram(positive_sid_to_success_rate_dict, EXPERIMENT_NAME, f\"Distribution of Positive Success Rates {PROCESS_TYPE}\")\n",
    "draw_outcome_bar(negative_sid_to_success_rate_dict,EXPERIMENT_NAME, f\"Negative Accuracy {PROCESS_TYPE}\")\n",
    "draw_outcome_histogram(negative_sid_to_success_rate_dict, EXPERIMENT_NAME, f\"Distribution of Negative Success Rates {PROCESS_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "load_dotenv()\n",
    "\n",
    "system_prompt = '''\n",
    "Please find a regular expression to match all packet payloads.\n",
    "You need to find the similarities in the sentences and generalize the parts where they differ. \n",
    "The regular expression is in PCRE format, please be aware to evaluate the validity of the expression you generated under PCRE regulations. \n",
    "There will be examples to help you find the patterns. \n",
    "[‘DELE 3\\r\\n’, ‘DELE 128\\r\\n’, ‘DELE 74\\r\\n’, ‘DELE 22\\r\\n’, ‘DELE 70\\r\\n’] \n",
    "These examples show the attacker is trying to delete someone’s email by POP protocol. \n",
    "The index of the desired mail is indicated under the DELE command. \n",
    "Thus the best regular expression that matches them will be ‘^(DELE)( )(.*)(\\r\\n)$’ \n",
    "\n",
    "With the given example payloads: \n",
    "[‘EHLO BtuCBHdSb51.com\\r\\n’, ‘EHLO 203.187.87.27\\r\\n’, ‘EHLO slae02Fo9Ep.com\\r\\n’, ‘EHLO 210.64.37.51\\r\\n’, ‘EHLO LLb0RwqdbkikFWo.com\\r\\n’] \n",
    "These examples show the attacker is trying to make sure the SMTP server is up and running. The command EHLO works in both lower case and uppercase, after that follows the SMTP server address. \n",
    "Thus the best regular expression to match them will be ‘^([E|e][H|h][L|l][O|o])(.*)(\\r\\n)$’ \n",
    "\n",
    "note: Your output must be a in jason format，no need to write Markdown\n",
    "'''\n",
    "user_prompt = '''\n",
    "['DELE 107\\r\\n', 'DELE 124\\r\\n', 'DELE 113\\r\\n', 'DELE 7\\r\\n', 'DELE 105\\r\\n', 'DELE 71\\r\\n', 'DELE 122\\r\\n', 'DELE 25\\r\\n', 'DELE 61\\r\\n', 'DELE 100\\r\\n', 'DELE 65\\r\\n', 'DELE 52\\r\\n', 'DELE 69\\r\\n', 'DELE 43\\r\\n', 'DELE 21\\r\\n', 'DELE 67\\r\\n', 'DELE 90\\r\\n', 'DELE 14\\r\\n', 'DELE 109\\r\\n', 'DELE 77\\r\\n', 'DELE 73\\r\\n', 'DELE 104\\r\\n', 'DELE 45\\r\\n', 'DELE 10\\r\\n', 'DELE 89\\r\\n', 'DELE 3\\r\\n', 'DELE 120\\r\\n', 'DELE 96\\r\\n', 'DELE 48\\r\\n', 'DELE 95\\r\\n', 'DELE 110\\r\\n', 'DELE 12\\r\\n', 'DELE 27\\r\\n', 'DELE 56\\r\\n', 'DELE 34\\r\\n', 'DELE 55\\r\\n', 'DELE 132\\r\\n', 'DELE 38\\r\\n', 'DELE 26\\r\\n', 'DELE 86\\r\\n', 'DELE 115\\r\\n', 'DELE 103\\r\\n', 'DELE 17\\r\\n', 'DELE 101\\r\\n', 'DELE 8\\r\\n', 'DELE 11\\r\\n', 'DELE 33\\r\\n', 'DELE 53\\r\\n', 'DELE 88\\r\\n', 'DELE 76\\r\\n']\n",
    "Please give 3 possible and different regular expressions to match all of the elements. \n",
    "You can give only 1 expression if the 3 expressions you find are too similar. \n",
    "Let’s work this out in a step-by-step way to make sure we have the right answer. \n",
    "To make the expression not too general, make sure the expressions don’t match these negative examples: [‘CAPA\\r\\n’, ‘CAPA\\r\\n’, ‘\\x15\\x03\\x01’, ‘GET / HTTP/1.0\\r\\n\\r\\n’, ‘r\\n\\r\\n’] \n",
    "You only need to give me the three regular expressions in code format.\n",
    "'''\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=system_prompt,\n",
    ")\n",
    "\n",
    "response = model.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gmni\n",
    "load_dotenv()\n",
    "import google.generativeai as genai\n",
    "system_prompt = \"You are a professional Regular Expression maker.\"\n",
    "\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#獲取各個SID所對應到的text\n",
    "def map_sid_to_unique_texts(df):\n",
    "    sid_to_texts = {}\n",
    "    for _, row in df.iterrows():\n",
    "        sid = row['sid']\n",
    "        text = row['text']\n",
    "        if pd.notnull(text):  # Ensure text is not NaN\n",
    "            if sid in sid_to_texts:\n",
    "                sid_to_texts[sid].add(text)\n",
    "            else:\n",
    "                sid_to_texts[sid] = {text}\n",
    "    return sid_to_texts\n",
    "sid_to_unique_texts = map_sid_to_unique_texts(pop_df)\n",
    "#print(sid_to_unique_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#過濾掉text長度小於5的sid\n",
    "def filter_sids_by_text_length(sit_to_text_dict):\n",
    "    filtered_sids = set()\n",
    "    for key, value in sit_to_text_dict.items():\n",
    "        if len(value) > 5:  # Convert to string to avoid errors with non-string types\n",
    "            filtered_sids.add(key)\n",
    "    return filtered_sids\n",
    "\n",
    "# Assuming pop_df is your DataFrame\n",
    "filtered_sids = filter_sids_by_text_length(sid_to_unique_texts)\n",
    "#print(filtered_sids)\n",
    "#print(len(filtered_sids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcre_by_sid(sid):\n",
    "    with open('sid_table(packet).csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['SID'] == str(sid):\n",
    "                # Remove leading and trailing slashes from the pcre value\n",
    "                print (row['pcre'])\n",
    "                return row['pcre'].strip('/')\n",
    "    return None\n",
    "\n",
    "# Iterate Throush Positive payload text to see if the ans regex is correct    \n",
    "def is_ans_correct(sid):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = get_pcre_by_sid(str(sid))\n",
    "    print(f\"ansPattern: {ansPattern}\")\n",
    "    sid_to_unique_texts = map_sid_to_unique_texts(pop_df)\n",
    "    texts = sid_to_unique_texts[sid]\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        correct = correct + bool(regex.search(ansPattern, text,regex.DOTALL))\n",
    "        if not bool(regex.search(r'/^(GET|HEAD|POST|OPTIONS)( )(\\/|\\/version|\\/api.*|\\/jsproxy)( )(HTTP\\/1\\.(?:0|1))(\\r\\n)(.*)$/', text,regex.DOTALL)):\n",
    "            errorList.append(text)\n",
    "\n",
    "    print(\"answer test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    print(len(errorList))\n",
    "    for error in errorList:\n",
    "        print(f\"error: {error}\")\n",
    "    return correct, total\n",
    "is_ans_correct('1783777')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "CHAT_GPT_API_KEY = os.getenv('C_API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=CHAT_GPT_API_KEY\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "load_dotenv()\n",
    "\n",
    "system_prompt = '''\n",
    "Please find a regular expression to match all packet payloads.\n",
    "You need to find the similarities in the sentences and generalize the parts where they differ. \n",
    "The regular expression is in PCRE format, please be aware to evaluate the validity of the expression you generated under PCRE regulations. \n",
    "There will be examples to help you find the patterns. \n",
    "[‘DELE 3\\r\\n’, ‘DELE 128\\r\\n’, ‘DELE 74\\r\\n’, ‘DELE 22\\r\\n’, ‘DELE 70\\r\\n’] \n",
    "These examples show the attacker is trying to delete someone’s email by POP protocol. \n",
    "The index of the desired mail is indicated under the DELE command. \n",
    "Thus the best regular expression that matches them will be ‘^(DELE)( )(.*)(\\r\\n)$’ \n",
    "\n",
    "With the given example payloads: \n",
    "[‘EHLO BtuCBHdSb51.com\\r\\n’, ‘EHLO 203.187.87.27\\r\\n’, ‘EHLO slae02Fo9Ep.com\\r\\n’, ‘EHLO 210.64.37.51\\r\\n’, ‘EHLO LLb0RwqdbkikFWo.com\\r\\n’] \n",
    "These examples show the attacker is trying to make sure the SMTP server is up and running. The command EHLO works in both lower case and uppercase, after that follows the SMTP server address. \n",
    "Thus the best regular expression to match them will be ‘^([E|e][H|h][L|l][O|o])(.*)(\\r\\n)$’ \n",
    "\n",
    "note: Your output must be a in jason format，no need to write Markdown\n",
    "'''\n",
    "user_prompt = '''\n",
    "['DELE 107\\r\\n', 'DELE 124\\r\\n', 'DELE 113\\r\\n', 'DELE 7\\r\\n', 'DELE 105\\r\\n', 'DELE 71\\r\\n', 'DELE 122\\r\\n', 'DELE 25\\r\\n', 'DELE 61\\r\\n', 'DELE 100\\r\\n', 'DELE 65\\r\\n', 'DELE 52\\r\\n', 'DELE 69\\r\\n', 'DELE 43\\r\\n', 'DELE 21\\r\\n', 'DELE 67\\r\\n', 'DELE 90\\r\\n', 'DELE 14\\r\\n', 'DELE 109\\r\\n', 'DELE 77\\r\\n', 'DELE 73\\r\\n', 'DELE 104\\r\\n', 'DELE 45\\r\\n', 'DELE 10\\r\\n', 'DELE 89\\r\\n', 'DELE 3\\r\\n', 'DELE 120\\r\\n', 'DELE 96\\r\\n', 'DELE 48\\r\\n', 'DELE 95\\r\\n', 'DELE 110\\r\\n', 'DELE 12\\r\\n', 'DELE 27\\r\\n', 'DELE 56\\r\\n', 'DELE 34\\r\\n', 'DELE 55\\r\\n', 'DELE 132\\r\\n', 'DELE 38\\r\\n', 'DELE 26\\r\\n', 'DELE 86\\r\\n', 'DELE 115\\r\\n', 'DELE 103\\r\\n', 'DELE 17\\r\\n', 'DELE 101\\r\\n', 'DELE 8\\r\\n', 'DELE 11\\r\\n', 'DELE 33\\r\\n', 'DELE 53\\r\\n', 'DELE 88\\r\\n', 'DELE 76\\r\\n']\n",
    "Please give 3 possible and different regular expressions to match all of the elements. \n",
    "You can give only 1 expression if the 3 expressions you find are too similar. \n",
    "Let’s work this out in a step-by-step way to make sure we have the right answer. \n",
    "To make the expression not too general, make sure the expressions don’t match these negative examples: [‘CAPA\\r\\n’, ‘CAPA\\r\\n’, ‘\\x15\\x03\\x01’, ‘GET / HTTP/1.0\\r\\n\\r\\n’, ‘r\\n\\r\\n’] \n",
    "You only need to give me the three regular expressions in code format.\n",
    "'''\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=system_prompt,\n",
    ")\n",
    "\n",
    "response = model.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_string(s):\n",
    "    # 使用正则表达式移除字符串前后的 '''json 和 '''\n",
    "    cleaned_string = regex.sub(r\"^'''json|'''$\", '', s, flags=regex.MULTILINE).strip()\n",
    "    return cleaned_string\n",
    "output_string = clean_json_string(input_string)\n",
    "output_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Generated Regex\n",
    "Randonly take 20 positive and 20 negateve data as input to generated regex and answer regex, comparing there result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#隨機選擇除了給定的SID以外的100個text\n",
    "def select_random_texts(sid_to_unique_texts, given_sid, num_texts=100):\n",
    "    # Filter out the given SID\n",
    "    filtered_texts = [texts for sid, texts in sid_to_unique_texts.items() if sid != given_sid]\n",
    "    \n",
    "    # Flatten the list of lists to a single list of texts\n",
    "    all_texts = [text for sublist in filtered_texts for text in sublist]\n",
    "    \n",
    "    # Randomly select 100 texts, or all texts if there are fewer than 100\n",
    "    selected_texts = random.sample(all_texts, min(len(all_texts), num_texts))\n",
    "    \n",
    "    return selected_texts\n",
    "\n",
    "def get_pcre_by_sid(sid):\n",
    "    with open('sid_table(packet).csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['SID'] == str(sid):\n",
    "                # Remove leading and trailing slashes from the pcre value\n",
    "                return row['pcre']\n",
    "    return None\n",
    "\n",
    "def match_patterns(targetText, GeneratedPatternList,isPositive=True):\n",
    "    non_matching_patterns = []  # Step 1: Initialize list for non-matching patterns\n",
    "    # Check each pattern and add non-matching ones to the list\n",
    "    for i, pattern in enumerate(GeneratedPatternList):\n",
    "        if not regex.search(pattern, targetText, regex.DOTALL):\n",
    "            non_matching_patterns.append(f\"Pattern {i+1}: {pattern}\")\n",
    "    ourResult = True if (9-len(non_matching_patterns)) >= 7 else False\n",
    "    with open('evaluation_result.txt', 'w') as file:\n",
    "        for pattern in non_matching_patterns:\n",
    "            file.write(pattern)\n",
    "    #ansMatch = bool(regex.search(ansPattern, targetText))\n",
    "    return ourResult == isPositive\n",
    "\n",
    "def positive_answer_evaluation(threeAnsPattern,sid):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = get_pcre_by_sid(str(sid))\n",
    "    print(f\"ansPattern: {ansPattern}\")\n",
    "    sid_to_unique_texts = map_sid_to_unique_texts(pop_df)\n",
    "    texts = sid_to_unique_texts[sid]\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        correct = correct + bool(regex.search(ansPattern, text, regex.DOTALL))\n",
    "        if not match_patterns(text, threeAnsPattern, True):\n",
    "            errorList.append(text)\n",
    "\n",
    "    print(\"positive test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    print(f\"errorList: {errorList}\")\n",
    "    return correct, total\n",
    "def negative_answer_evaluation(GeneratedPatternList,sid):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = get_pcre_by_sid(str(sid))\n",
    "    texts = select_random_texts(sid_to_unique_texts, sid)\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        if(not regex.search(ansPattern, text, regex.DOTALL)):\n",
    "            correct = correct + 1\n",
    "        if  not match_patterns(text, GeneratedPatternList, False):\n",
    "            errorList.append(text)\n",
    "    print(\"negative test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    for error_text in errorList:\n",
    "        print(error_text)\n",
    "    print(f\"errorList: {errorList}\")\n",
    "    \n",
    "def positive_evaluation(threeAnsPattern,sid,file):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = get_pcre_by_sid(str(sid))\n",
    "    print(f\"ansPattern: {ansPattern}\")\n",
    "    sid_to_unique_texts = map_sid_to_unique_texts(pop_df)\n",
    "    texts = sid_to_unique_texts[sid]\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        correct = correct + match_patterns(text, threeAnsPattern, True)\n",
    "        if not match_patterns(text, threeAnsPattern, True):\n",
    "            errorList.append(text)\n",
    "\n",
    "    print(\"positive test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    file.write('print(\"positive\")\\n')\n",
    "    for error in errorList:\n",
    "        file.write('print(f\"errorList: {errorList}\")\\n')\n",
    "    return correct, total\n",
    "\n",
    "def negative_evaluation(GeneratedPatternList,sid,file):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = get_pcre_by_sid(str(sid))\n",
    "    texts = select_random_texts(sid_to_unique_texts, sid)\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        correct = correct + match_patterns(text, GeneratedPatternList, False)\n",
    "        if  not match_patterns(text, GeneratedPatternList, False):\n",
    "            errorList.append(text)\n",
    "    file.write('print(\"negative\")\\n')\n",
    "    for error in errorList:\n",
    "        file.write('print(f\"errorList: {errorList}\")\\n')\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    for error_text in errorList:\n",
    "        \n",
    "    \n",
    "regex1 = r\"/^([E|e][H|h][L|l][O|o])( [a-zA-Z0-9.-]+)?(\\r\\n)$/\"\n",
    "regex2 = r\"/^EHLO( [a-zA-Z0-9.-]+)?(\\r\\n)$/\"\n",
    "regex3 = r\"/^([Ee][Hh][Ll][Oo])( [a-zA-Z0-9.-]+)?(\\r\\n)$/\"\n",
    "regex4 = r\"^([E|e][H|h][L|l][O|o] [a-zA-Z0-9.-]+(\\.[a-zA-Z]{2,})?\\r\\n)$\"\n",
    "regex5 = r\"^([E|e][H|h][L|l][O|o] [a-zA-Z0-9.-]+(\\.[a-zA-Z0-9.-]+)+\\r\\n|[E|e][H|h][L|l][O|o]\\r\\n)$\"\n",
    "regex6 = r\"^([E|e][H|h][L|l][O|o]( [a-zA-Z0-9.-]+(\\.[a-zA-Z0-9.-]+)+)?\\r\\n)$\"\n",
    "regex7 = r\"^(EHLO|ehlo)( [^\\r\\n]+)?(\\r\\n)$\"\n",
    "regex8 = r\"^([E|e][H|h][L|l][O|o])( [a-zA-Z0-9.-]+)?(\\r\\n)$\"\n",
    "regex9 = r\"^([Ee][Hh][Ll][Oo])( [a-zA-Z0-9.-]+)?(\\r\\n)$\"\n",
    "GeneratedPatternList = [\n",
    "    regex1, regex2, regex3, regex4, regex5, regex6, regex7, regex8, regex9\n",
    "]\n",
    "\n",
    "sid = '1454621'\n",
    "positive_evaluation(GeneratedPatternList, sid)\n",
    "negative_evaluation(GeneratedPatternList, sid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_csv_columns():\n",
    "    with open('sid_table(packet).csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        # Print all column names\n",
    "        print(reader.fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print filtered_sids and corresponding text\n",
    "for sid in filtered_sids:\n",
    "    print(f\"sid: {sid}, text: {sid_to_unique_texts[sid]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols = [\n",
    "    {'name': 'pop_df', 'data': pop_df},\n",
    "    {'name': 'imap_df', 'data': imap_df},\n",
    "    {'name': 'smtp_df', 'data': smtp_df},\n",
    "    {'name': 'sip_df', 'data': sip_df}\n",
    "]\n",
    "\n",
    "with open('prompt.txt', 'w') as file:\n",
    "    pass\n",
    "\n",
    "with open('prompt.txt', 'a') as file:\n",
    "    for protocol in protocols:\n",
    "        protocol_name = protocol['name']\n",
    "        protocol_data = protocol['data']\n",
    "        \n",
    "        sid_to_unique_texts = map_sid_to_unique_texts(protocol_data)\n",
    "        filtered_sids = filter_sids_by_text_length(sid_to_unique_texts)\n",
    "        \n",
    "        # Sort the filtered_sids to ensure the output is ordered by SID\n",
    "        sorted_filtered_sids = sorted(filtered_sids)\n",
    "        \n",
    "        file.write(f\"Protocol: {protocol_name}\\n\")\n",
    "        \n",
    "        for sid in sorted_filtered_sids:\n",
    "            texts = list(sid_to_unique_texts[sid])\n",
    "            random.shuffle(texts)\n",
    "            selected_texts = texts[:50]\n",
    "            file.write(f\"sid: {sid}, text: {selected_texts}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            file.write(\"Please find a regular expression to match all packet payloads.\\n\" + \n",
    "                       \"You need to find the similarities in the sentences and generalize the parts where they differ. \\n\" + \n",
    "                        \"The regular expression is in PCRE format, please be aware to evaluate the validity of the expression you generated under PCRE regulations. \\n\" + \n",
    "                        \"There will be examples to help you find the patterns. \\n\"  +\n",
    "                        \"[‘DELE 3\\\\r\\\\n’, ‘DELE 128\\\\r\\\\n’, ‘DELE 74\\\\r\\\\n’, ‘DELE 22\\\\r\\\\n’, ‘DELE 70\\\\r\\\\n’] \\n\" +\n",
    "                        \"These examples show the attacker is trying to delete someone’s email by POP protocol. \\n\" +\n",
    "                        \"The index of the desired mail is indicated under the DELE command. \\n\" +\n",
    "                        \"Thus the best regular expression that matches them will be ‘^(DELE)( )(.*)(\\\\r\\\\n)$’ \\n\" + \"\\n\" +\n",
    "                        \"With the given example payloads: \\n\" +\n",
    "                        \"[‘EHLO BtuCBHdSb51.com\\\\r\\\\n’, ‘EHLO 203.187.87.27\\\\r\\\\n’, ‘EHLO slae02Fo9Ep.com\\\\r\\\\n’, ‘EHLO 210.64.37.51\\\\r\\\\n’, ‘EHLO LLb0RwqdbkikFWo.com\\\\r\\\\n’] \\n\" + \n",
    "                        \"These examples show the attacker is trying to make sure the SMTP server is up and running. \" + \n",
    "                        \"The command EHLO works in both lower case and uppercase, after that follows the SMTP server address. \\n\" +\n",
    "                        \"Thus the best regular expression to match them will be ‘^([E|e][H|h][L|l][O|o])(.*)(\\\\r\\\\n)$’ \\n\" + \"\\n\" +\n",
    "                        \"Next, with the given payloads: \\n\")\n",
    "            file.write(f\"{selected_texts}\\n\")         \n",
    "            file.write(\"Please give 3 possible and different regular expressions to match all of the elements. \\n\" +\n",
    "                        \"You can give only 1 expression if the 3 expressions you find are too similar. \\n\" + \n",
    "                        \"Let’s work this out in a step-by-step way to make sure we have the right answer. \\n\" + \n",
    "                        \"To make the expression not too general, make sure the expressions don’t match these negative examples: [‘CAPA\\\\r\\\\n’, ‘CAPA\\\\r\\\\n’, ‘\\\\x15\\\\x03\\\\x01’, ‘GET / HTTP/1.0\\\\r\\\\n\\\\r\\\\n’, ‘r\\\\n\\\\r\\\\n’] \\n\" +\n",
    "                        \"You only need to give me the three regular expressions in code format.\\n\")\n",
    "            file.write(\"\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_texts.txt', 'a') as file:\n",
    "    for protocol in protocols:\n",
    "        protocol_name = protocol['name']\n",
    "        protocol_data = protocol['data']\n",
    "        \n",
    "        sid_to_unique_texts = map_sid_to_unique_texts(protocol_data)\n",
    "        filtered_sids = filter_sids_by_text_length(sid_to_unique_texts)\n",
    "        \n",
    "        file.write(f\"Protocol: {protocol_name}\\n\")\n",
    "        \n",
    "        for sid in filtered_sids:\n",
    "            file.write(f\"sid: {sid}, text: {sid_to_unique_texts[sid]}\\n\")\n",
    "                            \n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#獲取特定資料集的所有SID\n",
    "def get_unique_sids_list(df):\n",
    "    unique_sids = df[df['text'].notnull()]['sid'].unique()\n",
    "    return unique_sids\n",
    "sidList = get_unique_sids_list(pop_df)\n",
    "print(sidList)\n",
    "#print(len(sidList))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
