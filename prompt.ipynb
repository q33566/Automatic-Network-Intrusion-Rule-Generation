{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#獲取特定資料集的所有SID\n",
    "def get_unique_sids_list(df):\n",
    "    unique_sids = df[df['text'].notnull()]['sid'].unique()\n",
    "    return unique_sids\n",
    "sidList = get_unique_sids_list(pop_df)\n",
    "print(sidList)\n",
    "#print(len(sidList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def readPkl(file):\n",
    "    with open(file, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def readCsv(file):\n",
    "    data = pd.read_csv(file)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FileProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "def get_unique_sids_list(df):\n",
    "    unique_sids = df[df['text'].notnull()]['sid'].unique()\n",
    "    return unique_sids\n",
    "\n",
    "def map_sid_to_unique_texts(df):\n",
    "    sid_to_texts = {}\n",
    "    for _, row in df.iterrows():\n",
    "        sid = row['sid']\n",
    "        text = row['text']\n",
    "        if pd.notnull(text):  # 確保 text 不是 NaN\n",
    "            if sid in sid_to_texts:\n",
    "                sid_to_texts[sid].add(text)\n",
    "            else:\n",
    "                sid_to_texts[sid] = {text}\n",
    "    return sid_to_texts\n",
    "\n",
    "\n",
    "def merge_dataframes_to_dict(dfs):\n",
    "    \"\"\"\n",
    "    Merge multiple dataframes into a dictionary where each sid maps to a list of unique texts.\n",
    "\n",
    "    :param dfs: List of pandas DataFrames, each with columns 'sid' and 'text'\n",
    "    :return: Dictionary with sid as keys and list of unique texts as values\n",
    "    \"\"\"\n",
    "    merged_dict = {}\n",
    "\n",
    "    for df in dfs:\n",
    "        for _, row in df.iterrows():\n",
    "            sid = row['sid']\n",
    "            text = row['text']\n",
    "            if pd.notnull(text):  # Ensure text is not NaN\n",
    "                if sid not in merged_dict:\n",
    "                    merged_dict[sid] = set()  # Use a set to avoid duplicates\n",
    "                merged_dict[sid].add(text)\n",
    "\n",
    "    # Convert sets to lists\n",
    "    merged_dict = {sid: list(texts) for sid, texts in merged_dict.items()}\n",
    "    \n",
    "    return merged_dict\n",
    "\n",
    "def filter_sids_by_text_length(sid_to_texts_dict):\n",
    "    filtered_sid_to_texts_dict = set()\n",
    "    for key, value in sid_to_texts_dict.items():\n",
    "        if len(value) > 5:  # 確保字串長度超過 5\n",
    "            filtered_sid_to_texts_dict.add(key)\n",
    "    return filtered_sid_to_texts_dict\n",
    "\n",
    "def select_random_texts(sid_to_texts_dict, exclude_sid, num_texts=100):\n",
    "    # 過濾掉給定的 SID\n",
    "    filtered_texts = [texts for sid, texts in sid_to_texts_dict.items() if sid != exclude_sid]\n",
    "    \n",
    "    # 將多個列表平坦化為單一列表\n",
    "    all_texts = [text for sublist in filtered_texts for text in sublist]\n",
    "    \n",
    "    # 過濾掉 nan 值\n",
    "    all_texts = [text for text in all_texts if not (isinstance(text, float) and np.isnan(text))]\n",
    "    \n",
    "    # 隨機選擇 100 個文本，若總數少於 100 則選擇所有文本\n",
    "    selected_texts = random.sample(all_texts, min(len(all_texts), num_texts))\n",
    "    \n",
    "    return selected_texts\n",
    "\n",
    "def select_all_negative_texts(sid_to_texts_dict, exclude_sid, num_texts=100):\n",
    "    # 過濾掉給定的 SID\n",
    "    filtered_texts = [texts for sid, texts in sid_to_texts_dict.items() if sid != exclude_sid]\n",
    "    \n",
    "    # 將多個列表平坦化為單一列表\n",
    "    all_texts = [text for sublist in filtered_texts for text in sublist]\n",
    "    \n",
    "    # 過濾掉 nan 值\n",
    "    all_texts = [text for text in all_texts if not (isinstance(text, float) and np.isnan(text))]\n",
    "    \n",
    "    return all_texts\n",
    "\n",
    "def prompt_generator(pop_df, imap_df, smtp_df, sip_df):\n",
    "    protocols = [\n",
    "    {'name': 'pop_df', 'data': pop_df},\n",
    "    {'name': 'imap_df', 'data': imap_df},\n",
    "    {'name': 'smtp_df', 'data': smtp_df},\n",
    "    {'name': 'sip_df', 'data': sip_df}\n",
    "    ]\n",
    "\n",
    "    with open('prompt.txt', 'w') as file:\n",
    "        pass\n",
    "\n",
    "    with open('prompt.txt', 'a') as file:\n",
    "        for protocol in protocols:\n",
    "            protocol_name = protocol['name']\n",
    "            protocol_data = protocol['data']\n",
    "            \n",
    "            sid_to_unique_texts = map_sid_to_unique_texts(protocol_data)\n",
    "            filtered_sids = filter_sids_by_text_length(sid_to_unique_texts)\n",
    "            \n",
    "            # Sort the filtered_sids to ensure the output is ordered by SID\n",
    "            sorted_filtered_sids = sorted(filtered_sids)\n",
    "            \n",
    "            file.write(f\"Protocol: {protocol_name}\\n\")\n",
    "            \n",
    "            for sid in sorted_filtered_sids:\n",
    "                texts = list(sid_to_unique_texts[sid])\n",
    "                random.shuffle(texts)\n",
    "                selected_texts = texts[:50]\n",
    "                file.write(f\"sid: {sid}, text: {selected_texts}\\n\")\n",
    "                file.write(\"\\n\")\n",
    "                file.write(\"Please find a regular expression to match all packet payloads.\\n\" + \n",
    "                        \"You need to find the similarities in the sentences and generalize the parts where they differ. \\n\" + \n",
    "                            \"The regular expression is in PCRE format, please be aware to evaluate the validity of the expression you generated under PCRE regulations. \\n\" + \n",
    "                            \"There will be examples to help you find the patterns. \\n\"  +\n",
    "                            \"[‘DELE 3\\\\r\\\\n’, ‘DELE 128\\\\r\\\\n’, ‘DELE 74\\\\r\\\\n’, ‘DELE 22\\\\r\\\\n’, ‘DELE 70\\\\r\\\\n’] \\n\" +\n",
    "                            \"These examples show the attacker is trying to delete someone’s email by POP protocol. \\n\" +\n",
    "                            \"The index of the desired mail is indicated under the DELE command. \\n\" +\n",
    "                            \"Thus the best regular expression that matches them will be ‘^(DELE)( )(.*)(\\\\r\\\\n)$’ \\n\" + \"\\n\" +\n",
    "                            \"With the given example payloads: \\n\" +\n",
    "                            \"[‘EHLO BtuCBHdSb51.com\\\\r\\\\n’, ‘EHLO 203.187.87.27\\\\r\\\\n’, ‘EHLO slae02Fo9Ep.com\\\\r\\\\n’, ‘EHLO 210.64.37.51\\\\r\\\\n’, ‘EHLO LLb0RwqdbkikFWo.com\\\\r\\\\n’] \\n\" + \n",
    "                            \"These examples show the attacker is trying to make sure the SMTP server is up and running. \" + \n",
    "                            \"The command EHLO works in both lower case and uppercase, after that follows the SMTP server address. \\n\" +\n",
    "                            \"Thus the best regular expression to match them will be ‘^([E|e][H|h][L|l][O|o])(.*)(\\\\r\\\\n)$’ \\n\" + \"\\n\" +\n",
    "                            \"Next, with the given payloads: \\n\")\n",
    "                file.write(f\"{selected_texts}\\n\")         \n",
    "                file.write(\"Please give 3 possible and different regular expressions to match all of the elements. \\n\" +\n",
    "                            \"You can give only 1 expression if the 3 expressions you find are too similar. \\n\" + \n",
    "                            \"Let’s work this out in a step-by-step way to make sure we have the right answer. \\n\" + \n",
    "                            \"To make the expression not too general, make sure the expressions don’t match these negative examples: [‘CAPA\\\\r\\\\n’, ‘CAPA\\\\r\\\\n’, ‘\\\\x15\\\\x03\\\\x01’, ‘GET / HTTP/1.0\\\\r\\\\n\\\\r\\\\n’, ‘r\\\\n\\\\r\\\\n’] \\n\" +\n",
    "                            \"You only need to give me the three regular expressions in code format.\\n\")\n",
    "                file.write(\"\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "import regex\n",
    "from API.FileProcessor import select_random_texts, map_sid_to_unique_texts\n",
    "\n",
    "def getPcreAnsBySid(sid):\n",
    "    with open('sid_table(packet).csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['SID'] == str(sid):\n",
    "                # Remove leading and trailing slashes from the pcre value\n",
    "                return row['pcre']\n",
    "    return None\n",
    "\n",
    "def match_patterns(targetText, generatedPatternList,isPositive=True):\n",
    "    non_matching_patterns = []  # Step 1: Initialize list for non-matching patterns\n",
    "    # Check each pattern and add non-matching ones to the list\n",
    "    for i, pattern in enumerate(generatedPatternList):\n",
    "        # 确保 pattern 是字符串类型\n",
    "        if not isinstance(pattern, str):\n",
    "            raise TypeError(f\"Pattern at index {i} is not a string: {pattern} (type {type(pattern)})\")\n",
    "        # 确保 targetText 是字符串类型\n",
    "        if not isinstance(targetText, str):\n",
    "            raise TypeError(f\"targetText is not a string: {targetText} (type {type(targetText)})\")\n",
    "        if not regex.search(pattern, targetText, regex.DOTALL):\n",
    "            non_matching_patterns.append(f\"Pattern {i+1}: {pattern}\")\n",
    "    ourResult = True if (len(generatedPatternList)-len(non_matching_patterns))/len(generatedPatternList) >= 2/3 else False\n",
    "    #ansMatch = bool(regex.search(ansPattern, targetText))\n",
    "    return ourResult == isPositive\n",
    "\n",
    "def positive_answer_evaluation(threeAnsPattern,sid,df):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = getPcreAnsBySid(str(sid))\n",
    "    print(f\"ansPattern: {ansPattern}\")\n",
    "    sid_to_unique_texts = map_sid_to_unique_texts(df)\n",
    "    texts = sid_to_unique_texts[sid]\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        correct = correct + bool(regex.search(ansPattern, text, regex.DOTALL))\n",
    "        if not match_patterns(text, threeAnsPattern, True):\n",
    "            errorList.append(text)\n",
    "\n",
    "    print(\"positive test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    print(f\"errorList: {errorList}\")\n",
    "    return correct, total\n",
    "def negative_answer_evaluation(GeneratedPatternList,sid,sid_to_unique_texts_dict):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = getPcreAnsBySid(str(sid))\n",
    "    texts = select_random_texts(sid_to_unique_texts_dict, sid)\n",
    "    for text in texts:\n",
    "        # 确保 ansPattern 是字符串类型\n",
    "        if not isinstance(ansPattern, str):\n",
    "            raise TypeError(f\"ansPattern is not a string: {ansPattern} (type {type(ansPattern)})\")\n",
    "        # 确保 text 是字符串类型\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(f\"text is not a string: {text} (type {type(text)})\")\n",
    "        total = total + 1\n",
    "        if(not regex.search(ansPattern, text, regex.DOTALL)):\n",
    "            correct = correct + 1\n",
    "        if  not match_patterns(text, GeneratedPatternList, False):\n",
    "            errorList.append(text)\n",
    "    print(\"negative test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    for error_text in errorList:\n",
    "        print(error_text)\n",
    "    print(f\"errorList: {errorList}\")\n",
    "    \n",
    "def positive_evaluation(generatedPattern,sid,sid_to_unique_texts_dict, file):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = getPcreAnsBySid(str(sid))\n",
    "    print(f\"ansPattern: {ansPattern}\")\n",
    "    texts = sid_to_unique_texts_dict[sid]\n",
    "    for text in texts:\n",
    "        # 确保 ansPattern 是字符串类型\n",
    "        if not isinstance(ansPattern, str):\n",
    "            raise TypeError(f\"ansPattern is not a string: {ansPattern} (type {type(ansPattern)})\")\n",
    "        # 确保 text 是字符串类型\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(f\"text is not a string: {text} (type {type(text)})\")\n",
    "        total = total + 1\n",
    "        correct = correct + match_patterns(text, generatedPattern, True)\n",
    "        if not match_patterns(text, generatedPattern, True):\n",
    "            errorList.append(text)\n",
    "\n",
    "    print(\"positive test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    file.write('positive test\\n')\n",
    "    for error in errorList:\n",
    "        file.write(f\"errorList: {errorList}\")\n",
    "    return correct, total\n",
    "\n",
    "def negative_evaluation(GeneratedPatternList,sid,sid_to_unique_texts_dict, NEGATIVE_SAMPLE_NUMBER,file):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = getPcreAnsBySid(str(sid))\n",
    "    texts = select_all_negative_texts(sid_to_unique_texts_dict, sid,NEGATIVE_SAMPLE_NUMBER)\n",
    "    \n",
    "    \n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        correct = correct + match_patterns(text, GeneratedPatternList, False)\n",
    "        if  not match_patterns(text, GeneratedPatternList, False):\n",
    "            errorList.append(text)\n",
    "    print(\"negative test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    file.write('negative test\\n')\n",
    "    for error in errorList:\n",
    "        file.write(f\"errorList: {errorList}\")\n",
    "        file.write('\\n')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    texts = select_random_texts(sid_to_unique_texts_dict, \"1161912\",10)\n",
    "    for text in texts:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = readPkl(\"pop_report_with_tknscore_new.pkl\")\n",
    "imap_df = readPkl(\"imap_report_with_tknscore_new.pkl\")\n",
    "smtp_df = readPkl(\"smtp_report_with_tknscore_new.pkl\")\n",
    "sip_df = readPkl(\"sip_report_with_tknscore_new.pkl\")\n",
    "label_df = readCsv(\"sid_table(packet).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pop_df, imap_df, smtp_df, sip_df]\n",
    "sid_to_unique_texts_dict = merge_dataframes_to_dict(dfs)\n",
    "filted_sid = filter_sids_by_text_length(sid_to_unique_texts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexPatern = \"^POST /ipp HTTP/1\\.1\\r\\nHost: [\\d\\.]+:110\\r\\nUser-Agent: Mozilla/5\\.0 zgrab/0\\.x\\r\\nContent-Length: \\d{3}\\r\\nAccept: \\*/\\*\\r\\nContent-Type: application/ipp\\r\\nAccept-Encoding: gzip\\r\\n\\r\\n\\\\x02\\\\x01$\"\n",
    "text        = \"POST /ipp HTTP/1.1\\r\\nHost: 61.66.51.40:110\\r\\nUser-Agent: Mozilla/5.0 zgrab/0.x\\r\\nContent-Length: 144\\r\\nAccept: */*\\r\\nContent-Type: application/ipp\\r\\nAccept-Encoding: gzip\\r\\n\\r\\n\\x02\\x01\"\n",
    "print(bool(regex.search(regexPatern, text , regex.DOTALL)))\n",
    "print(bool(regex.search(\"^POST /ipp HTTP/1\\.1\\r\\nHost: [\\d\\.]+:110\\r\\nUser-Agent: Mozilla/5\\.0 zgrab/0\\.x\\r\\nContent-Length: \\d{3}\\r\\nAccept: \\*/\\*\\r\\nContent-Type: application/ipp\\r\\nAccept-Encoding: gzip\\r\\n\\r\\n\\\\x02\\\\x01$\", \"POST /ipp HTTP/1.1\\r\\nHost: 61.66.51.40:110\\r\\nUser-Agent: Mozilla/5.0 zgrab/0.x\\r\\nContent-Length: 144\\r\\nAccept: */*\\r\\nContent-Type: application/ipp\\r\\nAccept-Encoding: gzip\\r\\n\\r\\n\\x02\\x01\" , regex.DOTALL)))\n",
    "regexPatern2 = \"^POST /ipp HTTP/1\\.1\\\\r\\\\nHost: [\\d\\.]+:110\\\\r\\\\nUser-Agent: Mozilla\\/5\\.0 zgrab\\/0\\.x\\\\r\\\\nContent-Length: \\d+\\\\r\\\\nAccept: \\*\\/\\*\\\\r\\\\nContent-Type: application\\/ipp\\\\r\\\\nAccept-Encoding: gzip\\\\r\\\\n\\\\r\\\\n\\\\\\\\x02\\\\\\\\x01$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeneratedPatternList = [\n",
    "\"^ACK sip:\\+48221530179@[0-9a-zA-Z\\.\\-]+ SIP/2\\.0\\\\r\\\\nVia: SIP/2\\.0/TCP [0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+:[0-9]+;branch=z9hG4bK-[0-9a-zA-Z\\-\\.\\:]+;rport\\\\r\\\\nMax-Forwards: 70\\\\r\\\\nTo: sip:\\+48221530179@[0-9a-zA-Z\\.\\-]+\\\\r\\\\nFrom: \\\"lorna\\\"<sip:lorna@[0-9a-zA-Z\\.\\-]+>;tag=[0-9a-zA-Z]+\\\\r\\\\nCall-ID: [0-9a-zA-Z_\\.]+\\\\r\\\\nCSeq: 1 ACK\\\\r\\\\nContent-Length: 0\\\\r\\\\n\\\\r\\\\n$\",\n",
    "\"^ACK sip:\\+48221530179@[0-9a-zA-Z\\.\\-]+ SIP/2\\.0\\\\r\\\\nVia: SIP/2\\.0/TCP [0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+:[0-9]+;branch=z9hG4bK-[0-9a-zA-Z\\-\\.\\:]+;rport\\\\r\\\\nMax-Forwards: 70\\\\r\\\\nTo: sip:\\+48221530179@[0-9a-zA-Z\\.\\-]+\\\\r\\\\nFrom: \\\"lorna\\\"<sip:lorna@[0-9a-zA-Z\\.\\-]+>;tag=[0-9a-zA-Z]+\\\\r\\\\nCall-ID: [0-9a-zA-Z_\\.]+\\\\r\\\\nCSeq: 1 ACK\\\\r\\\\nContent-Length: 0\\\\r\\\\n\\\\r\\\\n$\",\n",
    "\"^ACK sip:\\+48221530179@[0-9a-zA-Z\\.\\-]+ SIP/2\\.0\\\\r\\\\nVia: SIP/2\\.0/TCP [0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+:[0-9]+;branch=z9hG4bK-[0-9a-zA-Z\\-\\.\\:]+;rport\\\\r\\\\nMax-Forwards: 70\\\\r\\\\nTo: sip:\\+48221530179@[0-9a-zA-Z\\.\\-]+\\\\r\\\\nFrom: \\\"lorna\\\"<sip:lorna@[0-9a-zA-Z\\.\\-]+>;tag=[0-9a-zA-Z]+\\\\r\\\\nCall-ID: [0-9a-zA-Z_\\.]+\\\\r\\\\nCSeq: 1 ACK\\\\r\\\\nContent-Length: 0\\\\r\\\\n\\\\r\\\\n$\",\n",
    "\"/^ACK sip:\\+[0-9]+@[0-9a-zA-Z\\.\\-]+ SIP\\/2\\.0\\\\r\\\\nVia: SIP\\/2\\.0\\/TCP [0-9a-zA-Z\\.\\-:;]+\\\\r\\\\nMax-Forwards: 70\\\\r\\\\nTo: sip:\\+[0-9]+@[0-9a-zA-Z\\.\\-]+\\\\r\\\\nFrom: \\\"[^\\\"]+\\\"<sip:[^@]+@[0-9a-zA-Z\\.\\-]+>;tag=[0-9a-f]+\\\\r\\\\nCall-ID: [0-9a-zA-Z_\\.]+\\\\r\\\\nCSeq: 1 ACK\\\\r\\\\nContent-Length: 0\\\\r\\\\n\\\\r\\\\n$/\",\n",
    "\"/^ACK sip:\\+[0-9]+@[\\w\\.\\-]+ SIP\\/2\\.0\\\\r\\\\nVia: SIP\\/2\\.0\\/TCP [\\w\\.\\-:;]+\\\\r\\\\nMax-Forwards: 70\\\\r\\\\nTo: sip:\\+[0-9]+@[\\w\\.\\-]+\\\\r\\\\nFrom: \\\"[^\\\"]+\\\"<sip:[^@]+@[\\w\\.\\-]+>;tag=[\\da-f]+\\\\r\\\\nCall-ID: [\\w\\.\\-]+\\\\r\\\\nCSeq: 1 ACK\\\\r\\\\nContent-Length: 0\\\\r\\\\n\\\\r\\\\n$/\",\n",
    "\"/^ACK sip:\\+[0-9]+@[\\w\\.\\-]+ SIP\\/2\\.0\\\\r\\\\nVia: SIP\\/2\\.0\\/TCP [\\w\\.\\-:;]+\\\\r\\\\nMax-Forwards: 70\\\\r\\\\nTo: sip:\\+[0-9]+@[\\w\\.\\-]+\\\\r\\\\nFrom: \\\"[^\\\"]+\\\"<sip:[^@]+@[\\w\\.\\-]+>;tag=[0-9a-f]+\\\\r\\\\nCall-ID: [\\w\\.\\-]+\\\\r\\\\nCSeq: 1 ACK\\\\r\\\\nContent-Length: 0\\\\r\\\\n\\\\r\\\\n$/\",\n",
    "\"/^ACK\\s+sip:\\+\\d+@([a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}|(?:\\d{1,3}\\.){3}\\d{1,3})(:\\d+)?\\s+SIP\\/2\\.0\\r\\nVia: SIP\\/2\\.0\\/TCP \\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}:\\d+;branch=z9hG4bK-[a-zA-Z0-9-]+;rport\\r\\nMax-Forwards: 70\\r\\nTo: sip:\\+\\d+@\\1\\s*\\r\\nFrom: \\\"lorna\\\"<sip:lorna@\\1>;tag=[a-zA-Z0-9]+\\r\\nCall-ID: [a-zA-Z0-9_]+\\.\\.\\r\\nCSeq: 1 ACK\\r\\nContent-Length: 0\\r\\n\\r\\n$/\",\n",
    "\"/^ACK\\s+sip:\\+\\d+@([a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}|(?:\\d{1,3}\\.){3}\\d{1,3})\\s+SIP\\/2\\.0\\r\\nVia: SIP\\/2\\.0\\/TCP \\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}:\\d+;branch=z9hG4bK-[a-zA-Z0-9-]+;rport\\r\\nMax-Forwards: 70\\r\\nTo: sip:\\+\\d+@\\1\\r\\nFrom: \\\"lorna\\\"<sip:lorna@\\1>;tag=[a-zA-Z0-9]+\\r\\nCall-ID: [a-zA-Z0-9._-]+(?:\\.\\.)?\\r\\nCSeq: 1 ACK\\r\\nContent-Length: 0\\r\\n\\r\\n$/\",\n",
    "\"/^ACK\\s+sip:\\+\\d+@(?:\\d{1,3}\\.){3}\\d{1,3}\\s+SIP\\/2\\.0\\r\\nVia: SIP\\/2\\.0\\/TCP \\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}:\\d+;branch=z9hG4bK-[a-zA-Z0-9-]+;rport\\r\\nMax-Forwards: 70\\r\\nTo: sip:\\+\\d+@(?:\\d{1,3}\\.){3}\\d{1,3}\\r\\nFrom: \\\"lorna\\\"<sip:lorna@(?:\\d{1,3}\\.){3}\\d{1,3}>;tag=[a-zA-Z0-9]+\\r\\nCall-ID: [a-zA-Z0-9._-]+\\.\\.\\r\\nCSeq: 1 ACK\\r\\nContent-Length: 0\\r\\n\\r\\n$/\"                                                                                                                                                                                                                                                                                                            \n",
    "]\n",
    "\n",
    "#escaped_string = original_string.replace('\"', '\\\\\"')\n",
    "\n",
    "\n",
    "NEGATIVE_SAMPLE_NUMBER = 1000\n",
    "SID = '1602447'\n",
    "with open('test.txt', 'w') as file:\n",
    "    positive_evaluation(GeneratedPatternList, SID, sid_to_unique_texts_dict,file)\n",
    "    negative_evaluation(GeneratedPatternList, SID, sid_to_unique_texts_dict, NEGATIVE_SAMPLE_NUMBER,file)\n",
    "#後處理 \"前面加\\\n",
    "#把首尾的/去掉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#獲取各個SID所對應到的text\n",
    "def map_sid_to_unique_texts(df):\n",
    "    sid_to_texts = {}\n",
    "    for _, row in df.iterrows():\n",
    "        sid = row['sid']\n",
    "        text = row['text']\n",
    "        if pd.notnull(text):  # Ensure text is not NaN\n",
    "            if sid in sid_to_texts:\n",
    "                sid_to_texts[sid].add(text)\n",
    "            else:\n",
    "                sid_to_texts[sid] = {text}\n",
    "    return sid_to_texts\n",
    "sid_to_unique_texts = map_sid_to_unique_texts(pop_df)\n",
    "#print(sid_to_unique_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#過濾掉text長度小於5的sid\n",
    "def filter_sids_by_text_length(sit_to_text_dict):\n",
    "    filtered_sids = set()\n",
    "    for key, value in sit_to_text_dict.items():\n",
    "        if len(value) > 5:  # Convert to string to avoid errors with non-string types\n",
    "            filtered_sids.add(key)\n",
    "    return filtered_sids\n",
    "\n",
    "# Assuming pop_df is your DataFrame\n",
    "filtered_sids = filter_sids_by_text_length(sid_to_unique_texts)\n",
    "#print(filtered_sids)\n",
    "#print(len(filtered_sids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcre_by_sid(sid):\n",
    "    with open('sid_table(packet).csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['SID'] == str(sid):\n",
    "                # Remove leading and trailing slashes from the pcre value\n",
    "                print (row['pcre'])\n",
    "                return row['pcre'].strip('/')\n",
    "    return None\n",
    "\n",
    "# Iterate Throush Positive payload text to see if the ans regex is correct    \n",
    "def is_ans_correct(sid):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = get_pcre_by_sid(str(sid))\n",
    "    print(f\"ansPattern: {ansPattern}\")\n",
    "    sid_to_unique_texts = map_sid_to_unique_texts(pop_df)\n",
    "    texts = sid_to_unique_texts[sid]\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        correct = correct + bool(regex.search(ansPattern, text,regex.DOTALL))\n",
    "        if not bool(regex.search(r'/^(GET|HEAD|POST|OPTIONS)( )(\\/|\\/version|\\/api.*|\\/jsproxy)( )(HTTP\\/1\\.(?:0|1))(\\r\\n)(.*)$/', text,regex.DOTALL)):\n",
    "            errorList.append(text)\n",
    "\n",
    "    print(\"answer test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    print(len(errorList))\n",
    "    for error in errorList:\n",
    "        print(f\"error: {error}\")\n",
    "    return correct, total\n",
    "is_ans_correct('1783777')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "CHAT_GPT_API_KEY = os.getenv('C_API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=CHAT_GPT_API_KEY\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "load_dotenv()\n",
    "\n",
    "system_prompt = \"You are a cat. Your name is Neko.\"\n",
    "user_prompt = \"What is your name?\"\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model=genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-flash\",\n",
    "  system_instruction=system_prompt)\n",
    "\n",
    "response = model.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Generated Regex\n",
    "Randonly take 20 positive and 20 negateve data as input to generated regex and answer regex, comparing there result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#隨機選擇除了給定的SID以外的100個text\n",
    "def select_random_texts(sid_to_unique_texts, given_sid, num_texts=100):\n",
    "    # Filter out the given SID\n",
    "    filtered_texts = [texts for sid, texts in sid_to_unique_texts.items() if sid != given_sid]\n",
    "    \n",
    "    # Flatten the list of lists to a single list of texts\n",
    "    all_texts = [text for sublist in filtered_texts for text in sublist]\n",
    "    \n",
    "    # Randomly select 100 texts, or all texts if there are fewer than 100\n",
    "    selected_texts = random.sample(all_texts, min(len(all_texts), num_texts))\n",
    "    \n",
    "    return selected_texts\n",
    "\n",
    "def get_pcre_by_sid(sid):\n",
    "    with open('sid_table(packet).csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['SID'] == str(sid):\n",
    "                # Remove leading and trailing slashes from the pcre value\n",
    "                return row['pcre']\n",
    "    return None\n",
    "\n",
    "def match_patterns(targetText, GeneratedPatternList,isPositive=True):\n",
    "    non_matching_patterns = []  # Step 1: Initialize list for non-matching patterns\n",
    "    # Check each pattern and add non-matching ones to the list\n",
    "    for i, pattern in enumerate(GeneratedPatternList):\n",
    "        if not regex.search(pattern, targetText, regex.DOTALL):\n",
    "            non_matching_patterns.append(f\"Pattern {i+1}: {pattern}\")\n",
    "    ourResult = True if (9-len(non_matching_patterns)) >= 7 else False\n",
    "    with open('evaluation_result.txt', 'w') as file:\n",
    "        for pattern in non_matching_patterns:\n",
    "            file.write(pattern)\n",
    "    #ansMatch = bool(regex.search(ansPattern, targetText))\n",
    "    return ourResult == isPositive\n",
    "\n",
    "def positive_answer_evaluation(threeAnsPattern,sid):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = get_pcre_by_sid(str(sid))\n",
    "    print(f\"ansPattern: {ansPattern}\")\n",
    "    sid_to_unique_texts = map_sid_to_unique_texts(pop_df)\n",
    "    texts = sid_to_unique_texts[sid]\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        correct = correct + bool(regex.search(ansPattern, text, regex.DOTALL))\n",
    "        if not match_patterns(text, threeAnsPattern, True):\n",
    "            errorList.append(text)\n",
    "\n",
    "    print(\"positive test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    print(f\"errorList: {errorList}\")\n",
    "    return correct, total\n",
    "def negative_answer_evaluation(GeneratedPatternList,sid):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = get_pcre_by_sid(str(sid))\n",
    "    texts = select_random_texts(sid_to_unique_texts, sid)\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        if(not regex.search(ansPattern, text, regex.DOTALL)):\n",
    "            correct = correct + 1\n",
    "        if  not match_patterns(text, GeneratedPatternList, False):\n",
    "            errorList.append(text)\n",
    "    print(\"negative test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    for error_text in errorList:\n",
    "        print(error_text)\n",
    "    print(f\"errorList: {errorList}\")\n",
    "    \n",
    "def positive_evaluation(threeAnsPattern,sid,file):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = get_pcre_by_sid(str(sid))\n",
    "    print(f\"ansPattern: {ansPattern}\")\n",
    "    sid_to_unique_texts = map_sid_to_unique_texts(pop_df)\n",
    "    texts = sid_to_unique_texts[sid]\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        correct = correct + match_patterns(text, threeAnsPattern, True)\n",
    "        if not match_patterns(text, threeAnsPattern, True):\n",
    "            errorList.append(text)\n",
    "\n",
    "    print(\"positive test\")\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    file.write('print(\"positive\")\\n')\n",
    "    for error in errorList:\n",
    "        file.write('print(f\"errorList: {errorList}\")\\n')\n",
    "    return correct, total\n",
    "\n",
    "def negative_evaluation(GeneratedPatternList,sid,file):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    errorList = []\n",
    "    ansPattern = get_pcre_by_sid(str(sid))\n",
    "    texts = select_random_texts(sid_to_unique_texts, sid)\n",
    "    for text in texts:\n",
    "        total = total + 1\n",
    "        correct = correct + match_patterns(text, GeneratedPatternList, False)\n",
    "        if  not match_patterns(text, GeneratedPatternList, False):\n",
    "            errorList.append(text)\n",
    "    file.write('print(\"negative\")\\n')\n",
    "    for error in errorList:\n",
    "        file.write('print(f\"errorList: {errorList}\")\\n')\n",
    "    print(f\"correct: {correct}, total: {total}\")\n",
    "    for error_text in errorList:\n",
    "        \n",
    "    \n",
    "regex1 = r\"/^([E|e][H|h][L|l][O|o])( [a-zA-Z0-9.-]+)?(\\r\\n)$/\"\n",
    "regex2 = r\"/^EHLO( [a-zA-Z0-9.-]+)?(\\r\\n)$/\"\n",
    "regex3 = r\"/^([Ee][Hh][Ll][Oo])( [a-zA-Z0-9.-]+)?(\\r\\n)$/\"\n",
    "regex4 = r\"^([E|e][H|h][L|l][O|o] [a-zA-Z0-9.-]+(\\.[a-zA-Z]{2,})?\\r\\n)$\"\n",
    "regex5 = r\"^([E|e][H|h][L|l][O|o] [a-zA-Z0-9.-]+(\\.[a-zA-Z0-9.-]+)+\\r\\n|[E|e][H|h][L|l][O|o]\\r\\n)$\"\n",
    "regex6 = r\"^([E|e][H|h][L|l][O|o]( [a-zA-Z0-9.-]+(\\.[a-zA-Z0-9.-]+)+)?\\r\\n)$\"\n",
    "regex7 = r\"^(EHLO|ehlo)( [^\\r\\n]+)?(\\r\\n)$\"\n",
    "regex8 = r\"^([E|e][H|h][L|l][O|o])( [a-zA-Z0-9.-]+)?(\\r\\n)$\"\n",
    "regex9 = r\"^([Ee][Hh][Ll][Oo])( [a-zA-Z0-9.-]+)?(\\r\\n)$\"\n",
    "GeneratedPatternList = [\n",
    "    regex1, regex2, regex3, regex4, regex5, regex6, regex7, regex8, regex9\n",
    "]\n",
    "\n",
    "sid = '1454621'\n",
    "positive_evaluation(GeneratedPatternList, sid)\n",
    "negative_evaluation(GeneratedPatternList, sid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_csv_columns():\n",
    "    with open('sid_table(packet).csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        # Print all column names\n",
    "        print(reader.fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print filtered_sids and corresponding text\n",
    "for sid in filtered_sids:\n",
    "    print(f\"sid: {sid}, text: {sid_to_unique_texts[sid]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols = [\n",
    "    {'name': 'pop_df', 'data': pop_df},\n",
    "    {'name': 'imap_df', 'data': imap_df},\n",
    "    {'name': 'smtp_df', 'data': smtp_df},\n",
    "    {'name': 'sip_df', 'data': sip_df}\n",
    "]\n",
    "\n",
    "with open('prompt.txt', 'w') as file:\n",
    "    pass\n",
    "\n",
    "with open('prompt.txt', 'a') as file:\n",
    "    for protocol in protocols:\n",
    "        protocol_name = protocol['name']\n",
    "        protocol_data = protocol['data']\n",
    "        \n",
    "        sid_to_unique_texts = map_sid_to_unique_texts(protocol_data)\n",
    "        filtered_sids = filter_sids_by_text_length(sid_to_unique_texts)\n",
    "        \n",
    "        # Sort the filtered_sids to ensure the output is ordered by SID\n",
    "        sorted_filtered_sids = sorted(filtered_sids)\n",
    "        \n",
    "        file.write(f\"Protocol: {protocol_name}\\n\")\n",
    "        \n",
    "        for sid in sorted_filtered_sids:\n",
    "            texts = list(sid_to_unique_texts[sid])\n",
    "            random.shuffle(texts)\n",
    "            selected_texts = texts[:50]\n",
    "            file.write(f\"sid: {sid}, text: {selected_texts}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            file.write(\"Please find a regular expression to match all packet payloads.\\n\" + \n",
    "                       \"You need to find the similarities in the sentences and generalize the parts where they differ. \\n\" + \n",
    "                        \"The regular expression is in PCRE format, please be aware to evaluate the validity of the expression you generated under PCRE regulations. \\n\" + \n",
    "                        \"There will be examples to help you find the patterns. \\n\"  +\n",
    "                        \"[‘DELE 3\\\\r\\\\n’, ‘DELE 128\\\\r\\\\n’, ‘DELE 74\\\\r\\\\n’, ‘DELE 22\\\\r\\\\n’, ‘DELE 70\\\\r\\\\n’] \\n\" +\n",
    "                        \"These examples show the attacker is trying to delete someone’s email by POP protocol. \\n\" +\n",
    "                        \"The index of the desired mail is indicated under the DELE command. \\n\" +\n",
    "                        \"Thus the best regular expression that matches them will be ‘^(DELE)( )(.*)(\\\\r\\\\n)$’ \\n\" + \"\\n\" +\n",
    "                        \"With the given example payloads: \\n\" +\n",
    "                        \"[‘EHLO BtuCBHdSb51.com\\\\r\\\\n’, ‘EHLO 203.187.87.27\\\\r\\\\n’, ‘EHLO slae02Fo9Ep.com\\\\r\\\\n’, ‘EHLO 210.64.37.51\\\\r\\\\n’, ‘EHLO LLb0RwqdbkikFWo.com\\\\r\\\\n’] \\n\" + \n",
    "                        \"These examples show the attacker is trying to make sure the SMTP server is up and running. \" + \n",
    "                        \"The command EHLO works in both lower case and uppercase, after that follows the SMTP server address. \\n\" +\n",
    "                        \"Thus the best regular expression to match them will be ‘^([E|e][H|h][L|l][O|o])(.*)(\\\\r\\\\n)$’ \\n\" + \"\\n\" +\n",
    "                        \"Next, with the given payloads: \\n\")\n",
    "            file.write(f\"{selected_texts}\\n\")         \n",
    "            file.write(\"Please give 3 possible and different regular expressions to match all of the elements. \\n\" +\n",
    "                        \"You can give only 1 expression if the 3 expressions you find are too similar. \\n\" + \n",
    "                        \"Let’s work this out in a step-by-step way to make sure we have the right answer. \\n\" + \n",
    "                        \"To make the expression not too general, make sure the expressions don’t match these negative examples: [‘CAPA\\\\r\\\\n’, ‘CAPA\\\\r\\\\n’, ‘\\\\x15\\\\x03\\\\x01’, ‘GET / HTTP/1.0\\\\r\\\\n\\\\r\\\\n’, ‘r\\\\n\\\\r\\\\n’] \\n\" +\n",
    "                        \"You only need to give me the three regular expressions in code format.\\n\")\n",
    "            file.write(\"\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_texts.txt', 'a') as file:\n",
    "    for protocol in protocols:\n",
    "        protocol_name = protocol['name']\n",
    "        protocol_data = protocol['data']\n",
    "        \n",
    "        sid_to_unique_texts = map_sid_to_unique_texts(protocol_data)\n",
    "        filtered_sids = filter_sids_by_text_length(sid_to_unique_texts)\n",
    "        \n",
    "        file.write(f\"Protocol: {protocol_name}\\n\")\n",
    "        \n",
    "        for sid in filtered_sids:\n",
    "            file.write(f\"sid: {sid}, text: {sid_to_unique_texts[sid]}\\n\")\n",
    "\n",
    "        file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
